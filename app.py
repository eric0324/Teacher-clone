import streamlit as st
import json
import datetime
from streamlit_mermaid import st_mermaid

# å°å…¥è‡ªå®šç¾©æ¨¡å¡Š
from utils.config import load_config, get_env_variable, load_system_prompt
from utils.ui import setup_ui, display_chat_history, render_mermaid_diagrams
from utils.auth import check_password
from utils.knowledge import search_knowledge, extract_core_question_with_llm
from utils.llm_providers import (
    generate_openai_response, 
    generate_claude_response, 
    generate_deepseek_response,
    save_question_to_supabase,
    upload_file_to_anthropic
)

# è¨­ç½®é é¢é…ç½®å’Œæ¨™é¡Œ - ç§»é™¤å´é‚Šæ¬„é…ç½®
st.set_page_config(page_title="æ•¸ä½åˆ†èº«ç³»çµ±", layout="wide")

# åªéš±è—å´é‚Šæ¬„å’Œæ”¶èµ·ç®­é ­ï¼Œä¸å½±éŸ¿ä¸»è¦å…§å®¹
st.markdown("""
<style>
    /* éš±è—å´é‚Šæ¬„ */
    [data-testid="stSidebar"] {
        display: none !important;
        width: 0px !important;
    }
    
    /* éš±è—å´é‚Šæ¬„æ§åˆ¶æŒ‰éˆ• */
    [data-testid="stSidebarCollapsedControl"] {
        display: none !important;
        width: 0px !important;
    }
    
    /* éš±è—ç®­é ­æŒ‰éˆ• */
    section[data-testid="stSidebarContent"],
    div.st-emotion-cache-gsulwm,
    .st-emotion-cache-16j9m0,
    button[kind="headerNoPadding"] {
        display: none !important;
    }
    
    /* åªéš±è—å´é‚Šæ¬„çš„ç®­é ­åœ–æ¨™ï¼Œä¸å½±éŸ¿å…¶ä»–SVG */
    [data-testid="stSidebarCollapsedControl"] svg,
    button[data-testid="baseButton-headerNoPadding"] svg {
        display: none !important;
    }
    
    /* å°‡ PDF ä¸Šå‚³æŒ‰éˆ•å›ºå®šåœ¨åº•éƒ¨ */
    [data-testid="stFileUploader"] {
        position: fixed !important;
        bottom: 10px !important;
        left: 50px !important;
        right: 20px !important;
        background: white !important;
        padding: 10px 15px !important;
        z-index: 999 !important;
        width: auto !important;
        max-width: 600px !important;
        box-sizing: border-box !important;
        margin: 0 !important;
        border-radius: 8px !important;
        border: 1px solid #ddd !important;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1) !important;
    }
    
    /* èª¿æ•´èŠå¤©è¼¸å…¥æ¡†çš„ä½ç½®ï¼Œç‚º PDF ä¸Šå‚³æŒ‰éˆ•é¨°å‡ºç©ºé–“ */
    [data-testid="stBottom"] {
        bottom: 100px !important;
        z-index: 1000 !important;
    }
    
    /* ç¢ºä¿ä¸»è¦å…§å®¹å€åŸŸæœ‰è¶³å¤ çš„åº•éƒ¨é–“è· */
    .stMainBlockContainer {
        padding-bottom: 120px !important;
    }
    
    /* PDF ä¸Šå‚³æŒ‰éˆ•å…§éƒ¨æ¨£å¼èª¿æ•´ */
    [data-testid="stFileUploader"] [data-testid="stFileUploaderDropzone"] {
        min-height: 60px !important;
        padding: 12px !important;
        border-radius: 8px !important;
    }
    
    [data-testid="stFileUploader"] label {
        margin-bottom: 8px !important;
    }
</style>


""", unsafe_allow_html=True)

# æª¢æŸ¥ç™»å…¥ç‹€æ…‹
if not check_password():
    st.stop()  # å¦‚æœæœªç™»å…¥ï¼Œåœæ­¢æ‡‰ç”¨ç¨‹å¼åŸ·è¡Œ

# ä»¥ä¸‹æ˜¯æ‡‰ç”¨ç¨‹å¼ä¸»é«”éƒ¨åˆ†ï¼Œåªæœ‰åœ¨é€šéé©—è­‰å¾Œæ‰æœƒåŸ·è¡Œ
st.title("æ•¸ä½åˆ†èº«ç³»çµ± (Beta)")

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
if "messages" not in st.session_state:
    st.session_state.messages = []

if "use_persona" not in st.session_state:
    st.session_state.use_persona = False

if "prompt_name" not in st.session_state:
    st.session_state.prompt_name = "mj"

if "knowledge_table" not in st.session_state:
    st.session_state.knowledge_table = get_env_variable("KNOWLEDGE_TABLE", "knowledge_base")  

if "llm_provider" not in st.session_state:
    st.session_state.llm_provider = "claude"  # é è¨­ä½¿ç”¨ Claude

if "use_streaming" not in st.session_state:
    st.session_state.use_streaming = True  # é è¨­å•Ÿç”¨ä¸²æµå›æ‡‰

if "memory_length" not in st.session_state:
    st.session_state.memory_length = 3  # é™ä½é è¨­å€¼å¾ 5 åˆ° 3
    
# è¼‰å…¥é…ç½®
config = load_config()

# å°‡é…ç½®æ·»åŠ åˆ°æœƒè©±ç‹€æ…‹ä¸­
st.session_state.supabase = config.get("supabase")

# å†è¼‰å…¥ç³»çµ±æç¤ºè©
if "custom_prompt" not in st.session_state:
    st.session_state.custom_prompt = load_system_prompt(st.session_state.prompt_name)

# èŠå¤©ç•Œé¢éƒ¨åˆ†
st.subheader("æ•¸ä½åˆ†èº«èŠå¤©")

# æ·»åŠ ç·Šæ€¥æ§åˆ¶æŒ‰éˆ•
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    if st.button("ğŸ—‘ï¸ æ¸…é™¤èŠå¤©æ­·å² (è§£æ±º Token è¶…é™)", key="clear_chat_history"):
        st.session_state.messages = []
        st.success("èŠå¤©æ­·å²å·²æ¸…é™¤ï¼é€™æ‡‰è©²èƒ½è§£æ±º token è¶…é™å•é¡Œã€‚")
        st.rerun()

# æ·»åŠ  Token è¨ºæ–·åŠŸèƒ½
with st.expander("ğŸ” Token ä½¿ç”¨è¨ºæ–·"):
    if st.button("åˆ†æç•¶å‰ Token ä½¿ç”¨æƒ…æ³"):
        # è¨ˆç®—å„éƒ¨åˆ†çš„ token ä½¿ç”¨
        system_prompt = st.session_state.custom_prompt or ""
        system_tokens = len(system_prompt) / 2.5
        
        chat_history = st.session_state.messages[-st.session_state.memory_length*2:] if st.session_state.messages else []
        history_chars = sum(len(msg.get('content', '')) for msg in chat_history)
        history_tokens = history_chars / 2.5
        
        uploaded_file_id = st.session_state.get('uploaded_file_id', None)
        file_tokens = 120000 if uploaded_file_id else 0
        
        total_estimated = system_tokens + history_tokens + file_tokens
        
        st.write("ğŸ“Š **Token ä½¿ç”¨åˆ†æï¼š**")
        st.write(f"- ç³»çµ±æç¤ºè©ï¼š~{system_tokens:.0f} tokens ({len(system_prompt)} å­—ç¬¦)")
        st.write(f"- èŠå¤©æ­·å²ï¼š~{history_tokens:.0f} tokens ({history_chars} å­—ç¬¦)")
        st.write(f"- ä¸Šå‚³æª”æ¡ˆï¼š~{file_tokens} tokens")
        st.write(f"- **ç¸½è¨ˆä¼°ç®—ï¼š~{total_estimated:.0f} tokens**")
        
        if total_estimated > 180000:
            st.error(f"âš ï¸ é ä¼° tokens ({total_estimated:.0f}) è¶…éå®‰å…¨é™åˆ¶ï¼")
            st.write("**å»ºè­°è§£æ±ºæ–¹æ¡ˆï¼š**")
            if file_tokens > 0:
                st.write("1. æ¸…é™¤ä¸Šå‚³çš„æª”æ¡ˆ")
            if history_tokens > 50000:
                st.write("2. æ¸…é™¤èŠå¤©æ­·å²")
            if system_tokens > 20000:
                st.write("3. ç¸®çŸ­ç³»çµ±æç¤ºè©")
        elif total_estimated > 150000:
            st.warning(f"âš ï¸ é ä¼° tokens ({total_estimated:.0f}) æ¥è¿‘é™åˆ¶")
        else:
            st.success(f"âœ… Token ä½¿ç”¨æ­£å¸¸ ({total_estimated:.0f}/200000)")

# è¨­ç½®UIæ¨£å¼
setup_ui()

# åˆå§‹åŒ–ç‹€æ…‹æ­·å²ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if "status_history" not in st.session_state:
    st.session_state.status_history = []

# é¡¯ç¤ºèŠå¤©æ­·å²
display_chat_history()

# å„²å­˜åŠŸèƒ½å·²ç§»è‡³ utils.llm_providers æ¨¡çµ„

# æœç´¢çŸ¥è­˜åº«
def search_knowledge_base(query, update_status):
    """å¾çŸ¥è­˜åº«ä¸­æœç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„ä¿¡æ¯"""
    update_status("æˆ‘æ­£åœ¨æ€è€ƒä½ çš„å•é¡Œ...")
    try:
        core_result = extract_core_question_with_llm(query)
        core_question = core_result.get("core_question", query)
        st.session_state.last_core_question = core_question
        st.session_state.last_keywords = core_result.get("keywords", [])
        
        update_status("æ­£åœ¨å¾çŸ¥è­˜åº«å°‹æ‰¾ç›¸é—œè³‡è¨Š...")
        try:
            knowledge_points = search_knowledge(core_question)
            st.session_state.last_knowledge_points = knowledge_points
            
            print(f"[DEBUG] ===== çŸ¥è­˜åº«æœç´¢çµæœ =====")
            print(f"[DEBUG] æ‰¾åˆ° {len(knowledge_points)} å€‹çŸ¥è­˜é»")
            
            # æº–å‚™çŸ¥è­˜é»ä¿¡æ¯
            knowledge_info = []
            total_knowledge_chars = 0
            max_knowledge_chars = 200000  # è¨­å®šæœ€å¤§çŸ¥è­˜åº«å…§å®¹å­—ç¬¦æ•¸é™åˆ¶
            
            for i, item in enumerate(knowledge_points):
                match_info = f"({item.get('match_type', 'æœªçŸ¥åŒ¹é…é¡å‹')}, ç›¸ä¼¼åº¦: {item.get('similarity', 0):.2f})"
                knowledge_text = f"æ¦‚å¿µ: {item['concept']} {match_info}\nè§£é‡‹: {item['explanation']}"
                
                # æª¢æŸ¥æ·»åŠ é€™å€‹çŸ¥è­˜é»æ˜¯å¦æœƒè¶…éé™åˆ¶
                if total_knowledge_chars + len(knowledge_text) > max_knowledge_chars:
                    print(f"[WARNING] çŸ¥è­˜åº«å…§å®¹é”åˆ°é™åˆ¶ï¼Œæˆªæ–·åœ¨ç¬¬ {i} å€‹çŸ¥è­˜é»")
                    break
                
                knowledge_info.append(knowledge_text)
                
                chars_count = len(knowledge_text)
                total_knowledge_chars += chars_count
                print(f"[DEBUG] çŸ¥è­˜é» {i+1}: {chars_count} å­—ç¬¦")
            
            # å°‡çŸ¥è­˜é»æ•´åˆç‚ºä¸Šä¸‹æ–‡
            context = "\n\n".join(knowledge_info)
            print(f"[DEBUG] çŸ¥è­˜åº«ä¸Šä¸‹æ–‡ç¸½é•·åº¦: {len(context)} å­—ç¬¦ (~{len(context)//2.5:.0f} tokens ä¼°ç®—)")
            
            if len(context) > 300000:  # é™ä½è­¦å‘Šé–¾å€¼
                print(f"[WARNING] çŸ¥è­˜åº«å…§å®¹å¾ˆå¤§ï¼Œå¯èƒ½æœƒå°è‡´ token è¶…é™")
            
            return context
            
        except Exception as e:
            update_status(f"æœç´¢çŸ¥è­˜åº«æ™‚å‡ºéŒ¯: {str(e)}")
            return None
    except Exception as e:
        update_status(f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

# ç”Ÿæˆå›ç­”
def generate_answer(query, context, update_status):
    """ä½¿ç”¨æœç´¢åˆ°çš„çŸ¥è­˜ç”Ÿæˆå›ç­”"""
    update_status("æ‰¾åˆ°äº†ç›¸é—œè³‡è¨Šï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...")
    try:
        # ç²å–ç³»çµ±æç¤ºè©
        system_content = st.session_state.custom_prompt
        
        # ç²å–æœ€è¿‘ memory_length æ¢å°è©±æ­·å²
        # æª”æ¡ˆæ¨¡å¼ä¸‹ç‚ºäº†é¿å… token è¶…é™ï¼Œæš«æ™‚ä¸ä½¿ç”¨èŠå¤©æ­·å²
        print(f"[WARNING] æª”æ¡ˆæ¨¡å¼ï¼šç‚ºé¿å… token è¶…é™ï¼Œè·³éèŠå¤©æ­·å²")
        chat_history = []  # å®Œå…¨æ¸…ç©ºèŠå¤©æ­·å²
        
        print(f"[DEBUG] æª”æ¡ˆæ¨¡å¼èŠå¤©æ­·å²ç¸½å­—ç¬¦æ•¸: 0 (~0 tokens ä¼°ç®—)")
        
        # æ§‹å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": system_content}]

        # æ·»åŠ ç•¶å‰å•é¡Œå’Œä¸Šä¸‹æ–‡
        augmented_prompt = f"""
        <message>
            {query}
        </message>

        <retrieved_knowledge>
            {context}
        </retrieved_knowledge>
        """
        
        messages.append({"role": "user", "content": augmented_prompt})
        
        # RAG æ¨¡å¼ä¸ä½¿ç”¨æª”æ¡ˆï¼Œé¿å… token æ¶ˆè€—éå¤§
        print(f"[DEBUG] ===== RAG æ¨¡å¼ï¼Œä¸ä½¿ç”¨æª”æ¡ˆ =====")
        
        # ç”Ÿæˆå›ç­”ï¼ˆRAG æ¨¡å¼ä¸å‚³é€æª”æ¡ˆ IDï¼‰
        return generate_response(messages, file_id=None)
    except Exception as e:
        error_message = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        update_status(error_message)
        # è¿”å›éŒ¯èª¤æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯ None
        return [error_message, "éŒ¯èª¤"]

def generate_response(messages, file_id=None):
    """æ ¹æ“šé¸å®šçš„LLMä¾›æ‡‰å•†ç”Ÿæˆå›ç­”"""
    llm_provider = st.session_state.llm_provider
    use_streaming = st.session_state.use_streaming
    
    if llm_provider == "openai":
        llm_model = get_env_variable("LLM_MODEL", "gpt-4o")
        response, _ = generate_openai_response(
            messages=messages,
            model=llm_model
        )
        return response, "ä¸²æµ"
    elif llm_provider == "claude":
        claude_model = get_env_variable("CLAUDE_MODEL", "claude-sonnet-4-20250514")
        response, _ = generate_claude_response(
            messages=messages,
            model=claude_model,
            file_id=file_id
        )
        return response, "ä¸²æµ"
    elif llm_provider == "deepseek":
        deepseek_model = get_env_variable("DEEPSEEK_MODEL", "deepseek-chat")
        response, _ = generate_deepseek_response(
            messages=messages,
            model_id=deepseek_model
        )
        return response, "ä¸²æµ"
    else:
        # é è¨­ä½¿ç”¨ Claude
        claude_model = get_env_variable("CLAUDE_MODEL", "claude-sonnet-4-20250514")
        response, _ = generate_claude_response(
            messages=messages,
            model=claude_model,
            file_id=file_id
        )
        return response, "ä¸²æµ"


def display_streaming_response(stream_response, message_placeholder):
    """é¡¯ç¤ºä¸²æµå›æ‡‰"""
    full_response = ""
    llm_provider = st.session_state.llm_provider
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦ä¸²(éŒ¯èª¤ä¿¡æ¯æˆ–éä¸²æµå›æ‡‰)
    if isinstance(stream_response, str):
        message_placeholder.markdown(stream_response)
        return stream_response
    
    if llm_provider == "openai":
        # è™•ç† OpenAI ä¸²æµ
        for chunk in stream_response:
            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                if hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content'):
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        # æª¢æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ mermaid åœ–è¡¨ä»£ç¢¼å¡Šï¼Œå¦‚æœæœ‰å°±æ¸²æŸ“
                        processed_content = render_mermaid_diagrams(full_response)
                        if "<MERMAID_CHART>" in processed_content:
                            # é‡æ–°æ¸²æŸ“æ•´å€‹å…§å®¹ï¼ŒåŒ…æ‹¬åœ–è¡¨
                            message_placeholder.empty()
                            with message_placeholder.container():
                                parts = processed_content.split("<MERMAID_CHART>")
                                for i, part in enumerate(parts):
                                    if i == 0:
                                        # ç¬¬ä¸€éƒ¨åˆ†æ˜¯ç´”æ–‡æœ¬
                                        if part:
                                            st.markdown(part, unsafe_allow_html=True)
                                    else:
                                        # æŸ¥æ‰¾åœ–è¡¨ä»£ç¢¼å’Œå¾ŒçºŒæ–‡æœ¬
                                        chart_end = part.find("</MERMAID_CHART>")
                                        if chart_end != -1:
                                            chart_code = part[:chart_end]
                                            remaining_text = part[chart_end + 16:]  # 16æ˜¯</MERMAID_CHART>çš„é•·åº¦
                                            
                                            # æ¸²æŸ“åœ–è¡¨
                                            try:
                                                st_mermaid(chart_code, height=350)
                                            except Exception as e:
                                                st.error(f"åœ–è¡¨æ¸²æŸ“å¤±æ•—: {str(e)}")
                                                st.code(chart_code, language="mermaid")
                                            
                                            # æ¸²æŸ“å‰©é¤˜æ–‡æœ¬
                                            if remaining_text:
                                                st.markdown(remaining_text, unsafe_allow_html=True)
                        else:
                            # æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥æ›´æ–°
                            message_placeholder.markdown(full_response)
                elif hasattr(chunk.choices[0], 'text'):
                    # èˆŠç‰ˆ API å¯èƒ½ä½¿ç”¨ text è€Œé content
                    content = chunk.choices[0].text
                    if content:
                        full_response += content
                        message_placeholder.markdown(full_response)
    
    elif llm_provider == "claude":
        # è™•ç† Claude ä¸²æµ
        for chunk in stream_response:
            # è™•ç†ä¸åŒé¡å‹çš„äº‹ä»¶å’Œçµæ§‹
            if hasattr(chunk, 'type'):
                # è™•ç†content_block_deltaäº‹ä»¶
                if chunk.type == 'content_block_delta' and hasattr(chunk, 'delta'):
                    if hasattr(chunk.delta, 'type') and chunk.delta.type == 'text_delta':
                        if hasattr(chunk.delta, 'text'):
                            content = chunk.delta.text
                            if content:
                                full_response += content
                                # æª¢æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ mermaid åœ–è¡¨ä»£ç¢¼å¡Šï¼Œå¦‚æœæœ‰å°±æ¸²æŸ“
                                processed_content = render_mermaid_diagrams(full_response)
                                if "<MERMAID_CHART>" in processed_content:
                                    # é‡æ–°æ¸²æŸ“æ•´å€‹å…§å®¹ï¼ŒåŒ…æ‹¬åœ–è¡¨
                                    message_placeholder.empty()
                                    with message_placeholder.container():
                                        parts = processed_content.split("<MERMAID_CHART>")
                                        for i, part in enumerate(parts):
                                            if i == 0:
                                                # ç¬¬ä¸€éƒ¨åˆ†æ˜¯ç´”æ–‡æœ¬
                                                if part:
                                                    st.markdown(part, unsafe_allow_html=True)
                                            else:
                                                # æŸ¥æ‰¾åœ–è¡¨ä»£ç¢¼å’Œå¾ŒçºŒæ–‡æœ¬
                                                chart_end = part.find("</MERMAID_CHART>")
                                                if chart_end != -1:
                                                    chart_code = part[:chart_end]
                                                    remaining_text = part[chart_end + 16:]  # 16æ˜¯</MERMAID_CHART>çš„é•·åº¦
                                                    
                                                    # æ¸²æŸ“åœ–è¡¨
                                                    try:
                                                        st_mermaid(chart_code, height=350)
                                                    except Exception as e:
                                                        st.error(f"åœ–è¡¨æ¸²æŸ“å¤±æ•—: {str(e)}")
                                                        st.code(chart_code, language="mermaid")
                                                    
                                                    # æ¸²æŸ“å‰©é¤˜æ–‡æœ¬
                                                    if remaining_text:
                                                        st.markdown(remaining_text, unsafe_allow_html=True)
                                else:
                                    # æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥æ›´æ–°
                                    message_placeholder.markdown(full_response)
                # Claude 2.x èˆŠç‰ˆAPI
                elif chunk.type == 'completion' and hasattr(chunk, 'completion'):
                    content = chunk.completion
                    if content:
                        full_response += content
                        message_placeholder.markdown(full_response)
    
    elif llm_provider == "deepseek":
        # è™•ç† DeepSeek ä¸²æµ
        try:
            for line in stream_response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: ') and line != 'data: [DONE]':
                        json_data = json.loads(line[6:])
                        if 'choices' in json_data and json_data['choices'] and 'delta' in json_data['choices'][0]:
                            content = json_data['choices'][0]['delta'].get('content', '')
                            if content:
                                full_response += content
                                # æª¢æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ mermaid åœ–è¡¨ä»£ç¢¼å¡Šï¼Œå¦‚æœæœ‰å°±æ¸²æŸ“
                                processed_content = render_mermaid_diagrams(full_response)
                                if "<MERMAID_CHART>" in processed_content:
                                    # é‡æ–°æ¸²æŸ“æ•´å€‹å…§å®¹ï¼ŒåŒ…æ‹¬åœ–è¡¨
                                    message_placeholder.empty()
                                    with message_placeholder.container():
                                        parts = processed_content.split("<MERMAID_CHART>")
                                        for i, part in enumerate(parts):
                                            if i == 0:
                                                # ç¬¬ä¸€éƒ¨åˆ†æ˜¯ç´”æ–‡æœ¬
                                                if part:
                                                    st.markdown(part, unsafe_allow_html=True)
                                            else:
                                                # æŸ¥æ‰¾åœ–è¡¨ä»£ç¢¼å’Œå¾ŒçºŒæ–‡æœ¬
                                                chart_end = part.find("</MERMAID_CHART>")
                                                if chart_end != -1:
                                                    chart_code = part[:chart_end]
                                                    remaining_text = part[chart_end + 16:]  # 16æ˜¯</MERMAID_CHART>çš„é•·åº¦
                                                    
                                                    # æ¸²æŸ“åœ–è¡¨
                                                    try:
                                                        st_mermaid(chart_code, height=350)
                                                    except Exception as e:
                                                        st.error(f"åœ–è¡¨æ¸²æŸ“å¤±æ•—: {str(e)}")
                                                        st.code(chart_code, language="mermaid")
                                                    
                                                    # æ¸²æŸ“å‰©é¤˜æ–‡æœ¬
                                                    if remaining_text:
                                                        st.markdown(remaining_text, unsafe_allow_html=True)
                                else:
                                    # æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥æ›´æ–°
                                    message_placeholder.markdown(full_response)
        except AttributeError:
            # å¦‚æœæ²’æœ‰iter_linesæ–¹æ³•ï¼Œå¯èƒ½æ˜¯éŒ¯èª¤è¨Šæ¯
            if hasattr(stream_response, 'text'):
                message_placeholder.markdown(stream_response.text)
                return stream_response.text
            message_placeholder.markdown(str(stream_response))
            return str(stream_response)
    
    elif llm_provider == "gemini":
        # è™•ç† Gemini ä¸²æµ
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºå­—ç¬¦ä¸²æˆ–éŒ¯èª¤æ¶ˆæ¯
            if isinstance(stream_response, str):
                processed_content = render_mermaid_diagrams(stream_response)
                if "<MERMAID_CHART>" in processed_content:
                    # å¦‚æœåŒ…å«åœ–è¡¨ï¼Œéœ€è¦ç‰¹æ®Šè™•ç†
                    message_placeholder.empty()
                    with message_placeholder.container():
                        parts = processed_content.split("<MERMAID_CHART>")
                        for i, part in enumerate(parts):
                            if i == 0:
                                if part:
                                    st.markdown(part, unsafe_allow_html=True)
                            else:
                                chart_end = part.find("</MERMAID_CHART>")
                                if chart_end != -1:
                                    chart_code = part[:chart_end]
                                    remaining_text = part[chart_end + 16:]
                                    try:
                                        st_mermaid(chart_code, height=350)
                                    except Exception as e:
                                        st.error(f"åœ–è¡¨æ¸²æŸ“å¤±æ•—: {str(e)}")
                                        st.code(chart_code, language="mermaid")
                                    if remaining_text:
                                        st.markdown(remaining_text, unsafe_allow_html=True)
                else:
                    # æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥é¡¯ç¤º
                    message_placeholder.markdown(stream_response)
                return stream_response
                
            # æª¢æŸ¥æ˜¯å¦ç‚ºç”Ÿæˆå™¨å°è±¡
            if not hasattr(stream_response, '__iter__') or not callable(stream_response.__iter__):
                error_msg = f"ç„¡æ³•è™•ç† Gemini éŸ¿æ‡‰ï¼šæ”¶åˆ°éæµå¼éŸ¿æ‡‰å°è±¡ ({type(stream_response).__name__})"
                message_placeholder.markdown(error_msg)
                return error_msg
                
            # å®‰å…¨æª¢æŸ¥
            max_chunk_count = 1000  # æœ€å¤§å…è¨±çš„chunkæ•¸é‡
            chunk_count = 0
            max_time_without_update = 30  # æœ€å¤§ä¸æ›´æ–°æ™‚é–“é™åˆ¶ï¼ˆç§’ï¼‰
            import time
            last_update_time = time.time()
                
            for chunk in stream_response:
                # å®‰å…¨æª¢æŸ¥
                chunk_count += 1
                if chunk_count > max_chunk_count:
                    full_response += "\n\n[Gemini å›æ‡‰é•·ï¼Œå·²ç¶“è¢«æˆªæ–·]"
                    message_placeholder.markdown(full_response)
                    break
                    
                # æ›´æ–°æ™‚é–“æª¢æŸ¥
                current_time = time.time()
                if current_time - last_update_time > max_time_without_update:
                    full_response += "\n\n[Gemini å›æ‡‰è¶…æ™‚ï¼Œå·²ç¶“è‡ªå‹•åœæ­¢]"
                    message_placeholder.markdown(full_response)
                    break
                
                content = None
                # è™•ç† Gemini API çš„ä¸åŒéŸ¿æ‡‰æ ¼å¼
                if hasattr(chunk, 'text'):
                    content = chunk.text
                elif hasattr(chunk, 'parts'):
                    for part in chunk.parts:
                        if hasattr(part, 'text') and part.text:
                            content = part.text
                elif hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            if hasattr(candidate.content, 'parts'):
                                for part in candidate.content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        content = part.text
                
                # å¦‚æœæˆåŠŸæå–åˆ°å…§å®¹ï¼Œå‰‡æ›´æ–°é¡¯ç¤º
                if content:
                    last_update_time = time.time()  # æ›´æ–°æ™‚é–“æˆ³
                    full_response += content
                    processed_content = render_mermaid_diagrams(full_response)
                    if "<MERMAID_CHART>" in processed_content:
                        # é‡æ–°æ¸²æŸ“æ•´å€‹å…§å®¹ï¼ŒåŒ…æ‹¬åœ–è¡¨
                        message_placeholder.empty()
                        with message_placeholder.container():
                            parts = processed_content.split("<MERMAID_CHART>")
                            for i, part in enumerate(parts):
                                if i == 0:
                                    # ç¬¬ä¸€éƒ¨åˆ†æ˜¯ç´”æ–‡æœ¬
                                    if part:
                                        st.markdown(part, unsafe_allow_html=True)
                                else:
                                    # æŸ¥æ‰¾åœ–è¡¨ä»£ç¢¼å’Œå¾ŒçºŒæ–‡æœ¬
                                    chart_end = part.find("</MERMAID_CHART>")
                                    if chart_end != -1:
                                        chart_code = part[:chart_end]
                                        remaining_text = part[chart_end + 16:]  # 16æ˜¯</MERMAID_CHART>çš„é•·åº¦
                                        
                                        # æ¸²æŸ“åœ–è¡¨
                                        try:
                                            st_mermaid(chart_code, height=350)
                                        except Exception as e:
                                            st.error(f"åœ–è¡¨æ¸²æŸ“å¤±æ•—: {str(e)}")
                                            st.code(chart_code, language="mermaid")
                                        
                                        # æ¸²æŸ“å‰©é¤˜æ–‡æœ¬
                                        if remaining_text:
                                            st.markdown(remaining_text, unsafe_allow_html=True)
                    else:
                        # æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥æ›´æ–°
                        message_placeholder.markdown(full_response)
        except Exception as e:
            # æ•ç²æ‰€æœ‰å¯èƒ½çš„éŒ¯èª¤
            error_msg = f"Gemini ä¸²æµè™•ç†ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            message_placeholder.markdown(error_msg)
            
            # å¦‚æœéŒ¯èª¤æ˜¯å› ç‚ºæ”¶åˆ°äº†æ„å¤–çš„éŸ¿æ‡‰æ ¼å¼
            if "object is not iterable" in str(e) or "object is not an iterator" in str(e):
                message_placeholder.markdown("æ”¶åˆ°çš„ Gemini éŸ¿æ‡‰ä¸æ˜¯å¯è¿­ä»£çš„ä¸²æµæ ¼å¼ï¼Œè«‹ç¨å¾Œå†è©¦")
                
            # æœ€å¾Œå˜—è©¦é¡¯ç¤ºåŸå§‹éŸ¿æ‡‰ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if isinstance(stream_response, str):
                return stream_response
            else:
                try:
                    # å˜—è©¦æå–æœ‰ç”¨ä¿¡æ¯
                    response_info = "ç„¡æ³•è§£æéŸ¿æ‡‰"
                    if hasattr(stream_response, 'text'):
                        response_info = stream_response.text
                    elif hasattr(stream_response, 'candidates') and stream_response.candidates:
                        candidate = stream_response.candidates[0]
                        if hasattr(candidate, 'content') and candidate.content:
                            if hasattr(candidate.content, 'parts'):
                                part = candidate.content.parts[0]
                                if hasattr(part, 'text'):
                                    response_info = part.text
                    
                    message_placeholder.markdown(response_info)
                    return response_info
                except:
                    return error_msg
    
    return full_response


# åˆå§‹åŒ–å»ºè­°å•é¡ŒæŒ‰éˆ•ç‹€æ…‹
if "show_suggestion_buttons" not in st.session_state:
    st.session_state.show_suggestion_buttons = True

# æ·»åŠ ä¸€å€‹æœƒè©±ç‹€æ…‹è®Šé‡ä¾†ä¿å­˜ç”¨æˆ¶é¸æ“‡çš„å»ºè­°å•é¡Œ
if "suggestion_prompt" not in st.session_state:
    st.session_state.suggestion_prompt = None

# æª¢æŸ¥æ˜¯å¦æœ‰ä¾†è‡ªå»ºè­°å•é¡Œçš„æå•
prompt_from_suggestion = None
if st.session_state.suggestion_prompt:
    prompt_from_suggestion = st.session_state.suggestion_prompt
    st.session_state.suggestion_prompt = None  # æ¸…é™¤å»ºè­°å•é¡Œï¼Œé¿å…é‡è¤‡è™•ç†

# PDF æª”æ¡ˆä¸Šå‚³æŒ‰éˆ• - åªæœ‰åœ¨æ²’æœ‰æª”æ¡ˆæ™‚æ‰é¡¯ç¤º
if "uploaded_file" not in st.session_state:
    st.session_state.uploaded_file = None

# åªæœ‰åœ¨æ²’æœ‰ä¸Šå‚³æª”æ¡ˆæ™‚æ‰é¡¯ç¤ºä¸Šå‚³æŒ‰éˆ•
if st.session_state.uploaded_file is None:
    uploaded_pdf = st.file_uploader(
        "ä¸Šå‚³ PDF æª”æ¡ˆé€²è¡Œå°è©±",
        type=['pdf'],
        help="æ”¯æ´ PDF æ ¼å¼ã€‚æª”æ¡ˆå°‡è‡ªå‹•ä¸Šå‚³åˆ° Anthropic ä¸¦å¯ç”¨æ–¼å°è©±ã€‚"
    )
    
    # å¦‚æœæœ‰æª”æ¡ˆä¸Šå‚³ï¼Œä¿å­˜åˆ° session state ä¸¦ä¸Šå‚³åˆ° Anthropic
    if uploaded_pdf is not None:
        print(f"[DEBUG] ===== ç”¨æˆ¶ä¸Šå‚³æª”æ¡ˆ =====")
        print(f"[DEBUG] æª”æ¡ˆåç¨±: {uploaded_pdf.name}")
        print(f"[DEBUG] æª”æ¡ˆé¡å‹: {uploaded_pdf.type}")
        print(f"[DEBUG] æª”æ¡ˆå¤§å°: {uploaded_pdf.size} bytes")
        
        with st.spinner("æ­£åœ¨è™•ç†ä¸¦ä¸Šå‚³æª”æ¡ˆåˆ° Anthropic..."):
            # è®€å–æª”æ¡ˆå…§å®¹
            file_content = uploaded_pdf.read()
            print(f"[DEBUG] å·²è®€å–æª”æ¡ˆå…§å®¹ï¼Œå¤§å°: {len(file_content)} bytes")
            
            # ä¸Šå‚³åˆ° Anthropic Files API
            print(f"[DEBUG] é–‹å§‹å‘¼å« upload_file_to_anthropic...")
            file_id, error, page_info = upload_file_to_anthropic(file_content, uploaded_pdf.name)
            
            print(f"[DEBUG] ä¸Šå‚³çµæœ:")
            print(f"[DEBUG] - æª”æ¡ˆ ID: {file_id}")
            print(f"[DEBUG] - éŒ¯èª¤ä¿¡æ¯: {error}")
            print(f"[DEBUG] - é æ•¸ä¿¡æ¯: {page_info}")
            
            if file_id:
                st.session_state.uploaded_file = uploaded_pdf
                st.session_state.uploaded_file_id = file_id
                st.session_state.uploaded_file_page_info = page_info
                
                # æ ¹æ“šé æ•¸ä¿¡æ¯é¡¯ç¤ºä¸åŒçš„æˆåŠŸè¨Šæ¯
                if page_info and page_info.get('was_trimmed'):
                    st.warning(f"æª”æ¡ˆ {uploaded_pdf.name} åŸæœ‰ {page_info['total_pages']} é ï¼Œå·²è‡ªå‹•è£åˆ‡ç‚ºå‰ 50 é ä¸¦ä¸Šå‚³æˆåŠŸï¼")
                elif page_info and page_info.get('total_pages'):
                    st.success(f"æª”æ¡ˆ {uploaded_pdf.name} ({page_info['total_pages']} é ) ä¸Šå‚³æˆåŠŸï¼")
                else:
                    st.success(f"æª”æ¡ˆ {uploaded_pdf.name} ä¸Šå‚³æˆåŠŸï¼")
                
                st.rerun()
            else:
                st.error(f"æª”æ¡ˆä¸Šå‚³å¤±æ•—ï¼š{error}")
                # é‡ç½®æª”æ¡ˆä¸Šå‚³ç‹€æ…‹
                st.session_state.uploaded_file = None
                if "uploaded_file_id" in st.session_state:
                    del st.session_state.uploaded_file_id
else:
    # é¡¯ç¤ºå·²ä¸Šå‚³çš„æª”æ¡ˆä¿¡æ¯å’Œæ¸…é™¤æŒ‰éˆ•
    uploaded_pdf = st.session_state.uploaded_file
    
    # å‰µå»ºä¸€å€‹å›ºå®šåœ¨åº•éƒ¨çš„å·²ä¸Šå‚³æª”æ¡ˆé¡¯ç¤ºå€åŸŸ
    page_info = st.session_state.get('uploaded_file_page_info', {})
    
    # æ§‹å»ºé æ•¸ä¿¡æ¯æ–‡å­—
    page_text = ""
    if page_info.get('total_pages'):
        if page_info.get('was_trimmed'):
            page_text = f" - å·²è£åˆ‡ç‚ºå‰ 50 é  (åŸ {page_info['total_pages']} é )"
        else:
            page_text = f" - {page_info['total_pages']} é "
    
    st.markdown(f"""
    <div style="position: fixed; bottom: 10px; left: 50%; transform: translateX(-50%); 
                background: white; padding: 10px 15px; z-index: 1001; 
                width: auto; max-width: 500px; box-sizing: border-box; 
                border-radius: 8px; text-align: center;">
        <div>
            <span style="color: #28a745; font-weight: bold;">âœ“ å·²ä¸Šå‚³ï¼š</span>
            <span style="color: #333;">{uploaded_pdf.name}</span>
            <span style="color: #666; font-size: 0.9em;">({uploaded_pdf.size / 1024 / 1024:.1f} MB{page_text})</span>
        </div>
    </div>
    """, unsafe_allow_html=True)

# è¼¸å…¥æ¡† - å§‹çµ‚é¡¯ç¤ºè¼¸å…¥æ¡†
prompt_from_input = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")

# æ±ºå®šè¦ä½¿ç”¨å“ªå€‹æç¤º
prompt = prompt_from_suggestion if prompt_from_suggestion else prompt_from_input


# è™•ç†ç”¨æˆ¶è¼¸å…¥
if prompt:
    # ç•¶ç”¨æˆ¶è¼¸å…¥å•é¡Œæ™‚ï¼Œéš±è—å»ºè­°å•é¡ŒæŒ‰éˆ•
    st.session_state.show_suggestion_buttons = False
    
    # æ¸…ç©ºç‹€æ…‹æ­·å²
    st.session_state.status_history = []
    
    # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯åˆ°èŠå¤©è¨˜éŒ„
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ç«‹å³é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
    with st.chat_message("user"):
        st.write(prompt)
    
    # å‰µå»ºä¸€å€‹å›ºå®šä½ç½®çš„ç‹€æ…‹è¨Šæ¯
    status_msg = st.empty()
    
    # æª¢æŸ¥å¿…è¦çš„APIé‡‘é‘°æ˜¯å¦è¨­ç½®
    api_check_passed = False
    llm_provider = st.session_state.llm_provider
    
    if llm_provider == "openai" and not get_env_variable("OPENAI_API_KEY", ""):
        with st.chat_message("assistant"):
            st.error("è«‹è¨­ç½® OpenAI API Key æ‰èƒ½ä½¿ç”¨å°è©±åŠŸèƒ½ã€‚")
    elif llm_provider == "claude" and not get_env_variable("CLAUDE_API_KEY", ""):
        with st.chat_message("assistant"):
            st.error("è«‹è¨­ç½® Claude API Key æ‰èƒ½ä½¿ç”¨ Claude æ¨¡å‹ã€‚")
    elif llm_provider == "deepseek" and not get_env_variable("DEEPSEEK_API_KEY", ""):
        with st.chat_message("assistant"):
            st.error("è«‹è¨­ç½® DeepSeek API Key æ‰èƒ½ä½¿ç”¨ DeepSeek æ¨¡å‹ã€‚")
    else:
        api_check_passed = True
    
    if not api_check_passed:
        st.stop()  # å¦‚æœæ²’æœ‰è¨­ç½®APIé‡‘é‘°ï¼Œåœæ­¢åŸ·è¡Œ
    
    # æ›´æ–°ç‹€æ…‹è¨Šæ¯å‡½æ•¸
    def update_status(status):
        # ä¿å­˜ç‹€æ…‹æ­·å²
        if status not in st.session_state.status_history:
            st.session_state.status_history.append(status)
        
        # åœ¨å›ºå®šä½ç½®é¡¯ç¤ºç•¶å‰ç‹€æ…‹
        with status_msg.container():
            with st.chat_message("assistant"):
                st.write(status)
    
    # åˆå§‹ç‹€æ…‹
    update_status("æ€è€ƒä¸­...")
    
    # ä½¿ç”¨çŸ¥è­˜åº«æˆ–ç›´æ¥å›ç­”
    voyage_api_key = get_env_variable("VOYAGE_API_KEY", "")
    uploaded_file_id = st.session_state.get('uploaded_file_id', None)
    
    # ä¿®æ”¹é‚è¼¯ï¼šå¦‚æœæœ‰æª”æ¡ˆä¸Šå‚³ï¼Œå°±ä¸ä½¿ç”¨ RAG æœç´¢ï¼Œé¿å… token æ¶ˆè€—éå¤§
    if uploaded_file_id:
        # æœ‰æª”æ¡ˆä¸Šå‚³æ™‚ï¼Œç›´æ¥ä½¿ç”¨æª”æ¡ˆé€²è¡Œå°è©±ï¼Œä¸æœç´¢çŸ¥è­˜åº«
        update_status("æ­£åœ¨åˆ†ææ‚¨ä¸Šå‚³çš„æª”æ¡ˆä¸¦ç”Ÿæˆå›ç­”...")
        try:
            # ç²å–ç³»çµ±æç¤ºè©
            system_content = st.session_state.custom_prompt
            
            # ç²å–æœ€è¿‘ memory_length æ¢å°è©±æ­·å²
            # æª”æ¡ˆæ¨¡å¼ä¸‹ç‚ºäº†é¿å… token è¶…é™ï¼Œæš«æ™‚ä¸ä½¿ç”¨èŠå¤©æ­·å²
            print(f"[WARNING] æª”æ¡ˆæ¨¡å¼ï¼šç‚ºé¿å… token è¶…é™ï¼Œè·³éèŠå¤©æ­·å²")
            chat_history = []  # å®Œå…¨æ¸…ç©ºèŠå¤©æ­·å²
            
            print(f"[DEBUG] æª”æ¡ˆæ¨¡å¼èŠå¤©æ­·å²ç¸½å­—ç¬¦æ•¸: 0 (~0 tokens ä¼°ç®—)")
            
            # æ§‹å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_content}]

            # æ·»åŠ ç•¶å‰å•é¡Œ
            messages.append({"role": "user", "content": prompt})
            
            print(f"[DEBUG] ===== ä½¿ç”¨æª”æ¡ˆæ¨¡å¼ï¼Œè·³é RAG æœç´¢ =====")
            print(f"[DEBUG] æª”æ¡ˆ ID: {uploaded_file_id}")
            
            # ç”Ÿæˆå›ç­”
            response_result = generate_response(messages, file_id=uploaded_file_id)
        except Exception as e:
            error_message = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            update_status(error_message)
            response_result = [error_message, "éŒ¯èª¤"]
    elif st.session_state.supabase and voyage_api_key:
        # æ²’æœ‰æª”æ¡ˆä¸Šå‚³æ™‚ï¼Œä½¿ç”¨çŸ¥è­˜åº«æœç´¢
        update_status("æ­£åœ¨æœç´¢çŸ¥è­˜åº«...")
        context = search_knowledge_base(prompt, update_status)
        response_result = generate_answer(prompt, context, update_status)
    else:
        # æ—¢æ²’æœ‰æª”æ¡ˆä¹Ÿæ²’æœ‰çŸ¥è­˜åº«ï¼Œç›´æ¥ä½¿ç”¨ LLM å›ç­”
        update_status("æ­£åœ¨ç”Ÿæˆå›ç­”...")
        try:
            # ç²å–ç³»çµ±æç¤ºè©
            system_content = st.session_state.custom_prompt
            
            # ç²å–æœ€è¿‘ memory_length æ¢å°è©±æ­·å²
            chat_history = st.session_state.messages[-st.session_state.memory_length*2:] if len(st.session_state.messages) > 0 else []
            
            # æ™ºèƒ½æˆªæ–·èŠå¤©æ­·å²ï¼Œé¿å… token è¶…é™
            max_history_chars = 120000  # ä¸€èˆ¬å°è©±æ¨¡å¼çš„é™åˆ¶
            total_history_chars = 0
            filtered_chat_history = []
            
            # å¾æœ€æ–°çš„è¨Šæ¯é–‹å§‹ï¼Œå‘å‰æ·»åŠ åˆ°é”é™åˆ¶ç‚ºæ­¢
            for msg in reversed(chat_history):
                char_count = len(msg.get('content', ''))
                if total_history_chars + char_count > max_history_chars:
                    print(f"[WARNING] ä¸€èˆ¬å°è©±æ¨¡å¼ï¼šèŠå¤©æ­·å²é”åˆ°é™åˆ¶ï¼Œæˆªæ–·è¼ƒæ—©çš„è¨Šæ¯")
                    break
                filtered_chat_history.insert(0, msg)  # æ’å…¥åˆ°é–‹é ­ä¿æŒé †åº
                total_history_chars += char_count
            
            chat_history = filtered_chat_history
            print(f"[DEBUG] ä¸€èˆ¬å°è©±æ¨¡å¼èŠå¤©æ­·å²ç¸½å­—ç¬¦æ•¸: {total_history_chars} (~{total_history_chars//2.5:.0f} tokens ä¼°ç®—)")
            
            # æ§‹å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_content}]

            # æ·»åŠ æ­·å²è¨Šæ¯
            for message in chat_history:
                messages.append(message)

            # æ·»åŠ ç•¶å‰å•é¡Œ
            messages.append({"role": "user", "content": prompt})
            
            print(f"[DEBUG] ===== ä¸€èˆ¬å°è©±æ¨¡å¼ =====")
            
            # ç”Ÿæˆå›ç­”ï¼ˆä¸å‚³é€æª”æ¡ˆ IDï¼‰
            response_result = generate_response(messages, file_id=None)
        except Exception as e:
            error_message = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            update_status(error_message)
            response_result = [error_message, "éŒ¯èª¤"]
    
    # æª¢æŸ¥ response_result æ˜¯å¦ç‚º None æˆ–ç„¡æ•ˆ
    if response_result is None:
        with st.chat_message("assistant"):
            st.error("ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹é‡æ–°å˜—è©¦æˆ–è¯ç¹«ç®¡ç†å“¡")
        # æ·»åŠ éŒ¯èª¤æ¶ˆæ¯åˆ°èŠå¤©è¨˜éŒ„
        st.session_state.messages.append({"role": "assistant", "content": "ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹é‡æ–°å˜—è©¦"})
        st.rerun()
    
    stream_response = response_result[0]
    is_streaming = True
    
    # æ¸…ç©ºç‹€æ…‹è¨Šæ¯
    status_msg.empty()
    
    # é¡¯ç¤ºå›ç­”ï¼ˆä¸²æµæˆ–å®Œæ•´ï¼‰
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # è™•ç†ä¸²æµéŸ¿æ‡‰
        full_response = display_streaming_response(stream_response, message_placeholder)
        response_text = full_response  # ä¿å­˜å®Œæ•´çš„å›ç­”ç”¨æ–¼æ·»åŠ åˆ°èŠå¤©æ­·å²
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°èŠå¤©è¨˜éŒ„
    st.session_state.messages.append({"role": "assistant", "content": response_text})
    
    # å„²å­˜å•é¡Œå’Œå›ç­”åˆ° Supabase
    save_question_to_supabase(
        question=prompt,
        answer=response_text,
        prompt_name=st.session_state.prompt_name,
        knowledge_table=st.session_state.knowledge_table
    )
    
    # é‡æ–°è¼‰å…¥é é¢ä»¥é‡ç½®èŠå¤©ç•Œé¢
    st.rerun() 